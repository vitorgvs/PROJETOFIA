{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d8b0bf-a6ba-46e4-9a50-cd65a48e0ad5",
   "metadata": {},
   "source": [
    "## CRIANDO CONEX√ÉO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bfb86b5-e2c2-4fd5-96aa-1b967acdb05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d47c7a0-d435-477d-812e-7e4c1b6bb5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('AulaSpark').enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8937992-7ce2-4466-8fe0-6d1a3280443a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AulaSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x762cf8614490>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a398a164-6e4b-4bd1-bdc1-c83140e86fbc",
   "metadata": {},
   "source": [
    "## CRIANDO DATAFRAME MANUALMENTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ef193e-8bd1-4907-9691-8f1416763de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aluno = spark.createDataFrame([\n",
    "    {\"id\": 1, \"name\": \"Fabio\"},\n",
    "    {\"id\": 2, \"name\": \"Joao\"},\n",
    "    {\"id\": 3, \"name\": \"Fernando\"}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c60081-e999-4034-b85a-b4d872ce6641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tipo do objeto\n",
    "type(df_aluno)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87f4cd2-52ec-4fe9-81a4-793e1863ef29",
   "metadata": {},
   "source": [
    "## HELP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46313f60-6672-4586-b09b-04f5e7242528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrame in module pyspark.sql.dataframe object:\n",
      "\n",
      "class DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      " |  DataFrame(jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n",
      " |  \n",
      " |  A distributed collection of data grouped into named columns.\n",
      " |  \n",
      " |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
      " |  and can be created using various functions in :class:`SparkSession`::\n",
      " |  \n",
      " |      people = spark.read.parquet(\"...\")\n",
      " |  \n",
      " |  Once created, it can be manipulated using the various domain-specific-language\n",
      " |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
      " |  \n",
      " |  To select a column from the :class:`DataFrame`, use the apply method::\n",
      " |  \n",
      " |      ageCol = people.age\n",
      " |  \n",
      " |  A more concrete example::\n",
      " |  \n",
      " |      # To create DataFrame using SparkSession\n",
      " |      people = spark.read.parquet(\"...\")\n",
      " |      department = spark.read.parquet(\"...\")\n",
      " |  \n",
      " |      people.filter(people.age > 30).join(department, people.deptId == department.id) \\\n",
      " |        .groupBy(department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n",
      " |  \n",
      " |  .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  .. note: A DataFrame should only be created as described above. It should not be directly\n",
      " |      created via using the constructor.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrame\n",
      " |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n",
      " |      pyspark.sql.pandas.conversion.PandasConversionMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> pyspark.sql.column.Column\n",
      " |      Returns the :class:`Column` denoted by ``name``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.age).collect()\n",
      " |      [Row(age=2), Row(age=5)]\n",
      " |  \n",
      " |  __getitem__(self, item: Union[int, str, pyspark.sql.column.Column, List, Tuple]) -> Union[pyspark.sql.column.Column, ForwardRef('DataFrame')]\n",
      " |      Returns the column as a :class:`Column`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df['age']).collect()\n",
      " |      [Row(age=2), Row(age=5)]\n",
      " |      >>> df[ [\"name\", \"age\"]].collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      >>> df[ df.age > 3 ].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df[df[0] > 3].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  __init__(self, jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  agg(self, *exprs: Union[pyspark.sql.column.Column, Dict[str, str]]) -> 'DataFrame'\n",
      " |      Aggregate on the entire :class:`DataFrame` without groups\n",
      " |      (shorthand for ``df.groupBy().agg()``).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.agg({\"age\": \"max\"}).collect()\n",
      " |      [Row(max(age)=5)]\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.agg(F.min(df.age)).collect()\n",
      " |      [Row(min(age)=2)]\n",
      " |  \n",
      " |  alias(self, alias: str) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` with an alias set.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      alias : str\n",
      " |          an alias name to be set for the :class:`DataFrame`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import *\n",
      " |      >>> df_as1 = df.alias(\"df_as1\")\n",
      " |      >>> df_as2 = df.alias(\"df_as2\")\n",
      " |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
      " |      >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\")                 .sort(desc(\"df_as1.name\")).collect()\n",
      " |      [Row(name='Bob', name='Bob', age=5), Row(name='Alice', name='Alice', age=2)]\n",
      " |  \n",
      " |  approxQuantile(self, col: Union[str, List[str], Tuple[str]], probabilities: Union[List[float], Tuple[float]], relativeError: float) -> Union[List[float], List[List[float]]]\n",
      " |      Calculates the approximate quantiles of numerical columns of a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The result of this algorithm has the following deterministic bound:\n",
      " |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      " |      probability `p` up to error `err`, then the algorithm will return\n",
      " |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      " |      close to (p * N). More precisely,\n",
      " |      \n",
      " |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      " |      \n",
      " |      This method implements a variation of the Greenwald-Khanna\n",
      " |      algorithm (with some speed optimizations). The algorithm was first\n",
      " |      present in [[https://doi.org/10.1145/375663.375670\n",
      " |      Space-efficient Online Computation of Quantile Summaries]]\n",
      " |      by Greenwald and Khanna.\n",
      " |      \n",
      " |      Note that null values will be ignored in numerical columns before calculation.\n",
      " |      For columns only containing null values, an empty list is returned.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col: str, tuple or list\n",
      " |          Can be a single column name, or a list of names for multiple columns.\n",
      " |      \n",
      " |          .. versionchanged:: 2.2\n",
      " |             Added support for multiple columns.\n",
      " |      probabilities : list or tuple\n",
      " |          a list of quantile probabilities\n",
      " |          Each number must belong to [0, 1].\n",
      " |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      " |      relativeError : float\n",
      " |          The relative target precision to achieve\n",
      " |          (>= 0). If set to zero, the exact quantiles are computed, which\n",
      " |          could be very expensive. Note that values greater than 1 are\n",
      " |          accepted but give the same result as 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          the approximate quantiles at the given probabilities. If\n",
      " |          the input `col` is a string, the output is a list of floats. If the\n",
      " |          input `col` is a list or tuple of strings, the output is also a\n",
      " |          list, but each element in it is a list of floats, i.e., the output\n",
      " |          is a list of list of floats.\n",
      " |  \n",
      " |  cache(self) -> 'DataFrame'\n",
      " |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n",
      " |  \n",
      " |  checkpoint(self, eager: bool = True) -> 'DataFrame'\n",
      " |      Returns a checkpointed version of this :class:`DataFrame`. Checkpointing can be used to\n",
      " |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      " |      iterative algorithms where the plan may grow exponentially. It will be saved to files\n",
      " |      inside the checkpoint directory set with :meth:`SparkContext.setCheckpointDir`.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eager : bool, optional\n",
      " |          Whether to checkpoint this :class:`DataFrame` immediately\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental.\n",
      " |  \n",
      " |  coalesce(self, numPartitions: int) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
      " |      \n",
      " |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
      " |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
      " |      there will not be a shuffle, instead each of the 100 new partitions will\n",
      " |      claim 10 of the current partitions. If a larger number of partitions is requested,\n",
      " |      it will stay at the current number of partitions.\n",
      " |      \n",
      " |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
      " |      this may result in your computation taking place on fewer nodes than\n",
      " |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
      " |      you can call repartition(). This will add a shuffle step, but means the\n",
      " |      current upstream partitions will be executed in parallel (per whatever\n",
      " |      the current partitioning is).\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int\n",
      " |          specify the target number of partitions\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.coalesce(1).rdd.getNumPartitions()\n",
      " |      1\n",
      " |  \n",
      " |  colRegex(self, colName: str) -> pyspark.sql.column.Column\n",
      " |      Selects column based on the column name specified as a regex and returns it\n",
      " |      as :class:`Column`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      colName : str\n",
      " |          string, column name specified as a regex.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
      " |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
      " |      +----+\n",
      " |      |Col2|\n",
      " |      +----+\n",
      " |      |   1|\n",
      " |      |   2|\n",
      " |      |   3|\n",
      " |      +----+\n",
      " |  \n",
      " |  collect(self) -> List[pyspark.sql.types.Row]\n",
      " |      Returns all the records as a list of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  corr(self, col1: str, col2: str, method: Optional[str] = None) -> float\n",
      " |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      " |      Currently only supports the Pearson Correlation Coefficient.\n",
      " |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col1 : str\n",
      " |          The name of the first column\n",
      " |      col2 : str\n",
      " |          The name of the second column\n",
      " |      method : str, optional\n",
      " |          The correlation method. Currently only supports \"pearson\"\n",
      " |  \n",
      " |  count(self) -> int\n",
      " |      Returns the number of rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.count()\n",
      " |      2\n",
      " |  \n",
      " |  cov(self, col1: str, col2: str) -> float\n",
      " |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
      " |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col1 : str\n",
      " |          The name of the first column\n",
      " |      col2 : str\n",
      " |          The name of the second column\n",
      " |  \n",
      " |  createGlobalTempView(self, name: str) -> None\n",
      " |      Creates a global temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createGlobalTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from global_temp.people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |      True\n",
      " |  \n",
      " |  createOrReplaceGlobalTempView(self, name: str) -> None\n",
      " |      Creates or replaces a global temporary view using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"select * from global_temp.people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |      True\n",
      " |  \n",
      " |  createOrReplaceTempView(self, name: str) -> None\n",
      " |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createOrReplaceTempView(\"people\")\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      True\n",
      " |  \n",
      " |  createTempView(self, name: str) -> None\n",
      " |      Creates a local temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      True\n",
      " |  \n",
      " |  crossJoin(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Returns the cartesian product with another :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Right side of the cartesian product.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(\"age\", \"name\").collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df2.select(\"name\", \"height\").collect()\n",
      " |      [Row(name='Tom', height=80), Row(name='Bob', height=85)]\n",
      " |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n",
      " |      [Row(age=2, name='Alice', height=80), Row(age=2, name='Alice', height=85),\n",
      " |       Row(age=5, name='Bob', height=80), Row(age=5, name='Bob', height=85)]\n",
      " |  \n",
      " |  crosstab(self, col1: str, col2: str) -> 'DataFrame'\n",
      " |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      " |      table. The number of distinct values for each column should be less than 1e4. At most 1e6\n",
      " |      non-zero pair frequencies will be returned.\n",
      " |      The first column of each row will be the distinct values of `col1` and the column names\n",
      " |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      " |      Pairs that have no occurrences will have zero as their counts.\n",
      " |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col1 : str\n",
      " |          The name of the first column. Distinct items will make the first item of\n",
      " |          each row.\n",
      " |      col2 : str\n",
      " |          The name of the second column. Distinct items will make the column names\n",
      " |          of the :class:`DataFrame`.\n",
      " |  \n",
      " |  cube(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
      " |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregations on them.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      | null|   2|    1|\n",
      " |      | null|   5|    1|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |  \n",
      " |  describe(self, *cols: Union[str, List[str]]) -> 'DataFrame'\n",
      " |      Computes basic statistics for numeric and string columns.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      This include count, mean, stddev, min, and max. If no columns are\n",
      " |      given, this function computes statistics for all numerical or string columns.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function is meant for exploratory data analysis, as we make no\n",
      " |      guarantee about the backward compatibility of the schema of the resulting\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      Use summary for expanded statistics and control over which statistics to compute.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
      " |      ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
      " |      ... )\n",
      " |      >>> df.describe(['age']).show()\n",
      " |      +-------+----+\n",
      " |      |summary| age|\n",
      " |      +-------+----+\n",
      " |      |  count|   3|\n",
      " |      |   mean|12.0|\n",
      " |      | stddev| 1.0|\n",
      " |      |    min|  11|\n",
      " |      |    max|  13|\n",
      " |      +-------+----+\n",
      " |      \n",
      " |      >>> df.describe(['age', 'weight', 'height']).show()\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |      |summary| age|            weight|           height|\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |      |  count|   3|                 3|                3|\n",
      " |      |   mean|12.0| 40.73333333333333|            145.0|\n",
      " |      | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
      " |      |    min|  11|              37.8|            142.2|\n",
      " |      |    max|  13|              44.1|            150.5|\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.summary\n",
      " |  \n",
      " |  distinct(self) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.distinct().count()\n",
      " |      2\n",
      " |  \n",
      " |  drop(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` that drops the specified column.\n",
      " |      This is a no-op if schema doesn't contain the given column name(s).\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols: str or :class:`Column`\n",
      " |          a name of the column, or the :class:`Column` to drop\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.drop('age').collect()\n",
      " |      [Row(name='Alice'), Row(name='Bob')]\n",
      " |      \n",
      " |      >>> df.drop(df.age).collect()\n",
      " |      [Row(name='Alice'), Row(name='Bob')]\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n",
      " |      [Row(age=5, height=85, name='Bob')]\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
      " |      [Row(age=5, name='Bob', height=85)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n",
      " |      [Row(name='Bob')]\n",
      " |  \n",
      " |  dropDuplicates(self, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
      " |      optionally only considering certain columns.\n",
      " |      \n",
      " |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      " |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      " |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      " |      be and system will accordingly limit the state. In addition, too late data older than\n",
      " |      watermark will be dropped to avoid any possibility of duplicates.\n",
      " |      \n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = sc.parallelize([ \\\n",
      " |      ...     Row(name='Alice', age=5, height=80), \\\n",
      " |      ...     Row(name='Alice', age=5, height=80), \\\n",
      " |      ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      " |      >>> df.dropDuplicates().show()\n",
      " |      +-----+---+------+\n",
      " |      | name|age|height|\n",
      " |      +-----+---+------+\n",
      " |      |Alice|  5|    80|\n",
      " |      |Alice| 10|    80|\n",
      " |      +-----+---+------+\n",
      " |      \n",
      " |      >>> df.dropDuplicates(['name', 'height']).show()\n",
      " |      +-----+---+------+\n",
      " |      | name|age|height|\n",
      " |      +-----+---+------+\n",
      " |      |Alice|  5|    80|\n",
      " |      +-----+---+------+\n",
      " |  \n",
      " |  drop_duplicates = dropDuplicates(self, subset=None)\n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  dropna(self, how: str = 'any', thresh: Optional[int] = None, subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      " |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      how : str, optional\n",
      " |          'any' or 'all'.\n",
      " |          If 'any', drop a row if it contains any nulls.\n",
      " |          If 'all', drop a row only if all its values are null.\n",
      " |      thresh: int, optional\n",
      " |          default None\n",
      " |          If specified, drop rows that have less than `thresh` non-null values.\n",
      " |          This overwrites the `how` parameter.\n",
      " |      subset : str, tuple or list, optional\n",
      " |          optional list of column names to consider.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df4.na.drop().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |  \n",
      " |  exceptAll(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
      " |      not in another :class:`DataFrame` while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT ALL` in SQL.\n",
      " |      As standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.createDataFrame(\n",
      " |      ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      \n",
      " |      >>> df1.exceptAll(df2).show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  a|  2|\n",
      " |      |  c|  4|\n",
      " |      +---+---+\n",
      " |  \n",
      " |  explain(self, extended: Union[bool, str, NoneType] = None, mode: Optional[str] = None) -> None\n",
      " |      Prints the (logical and physical) plans to the console for debugging purpose.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extended : bool, optional\n",
      " |          default ``False``. If ``False``, prints only the physical plan.\n",
      " |          When this is a string without specifying the ``mode``, it works as the mode is\n",
      " |          specified.\n",
      " |      mode : str, optional\n",
      " |          specifies the expected output format of plans.\n",
      " |      \n",
      " |          * ``simple``: Print only a physical plan.\n",
      " |          * ``extended``: Print both logical and physical plans.\n",
      " |          * ``codegen``: Print a physical plan and generated codes if they are available.\n",
      " |          * ``cost``: Print a logical plan and statistics if they are available.\n",
      " |          * ``formatted``: Split explain output into two sections: a physical plan outline                 and node details.\n",
      " |      \n",
      " |          .. versionchanged:: 3.0.0\n",
      " |             Added optional argument `mode` to specify the expected output format of plans.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.explain()\n",
      " |      == Physical Plan ==\n",
      " |      *(1) Scan ExistingRDD[age#0,name#1]\n",
      " |      \n",
      " |      >>> df.explain(True)\n",
      " |      == Parsed Logical Plan ==\n",
      " |      ...\n",
      " |      == Analyzed Logical Plan ==\n",
      " |      ...\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...\n",
      " |      == Physical Plan ==\n",
      " |      ...\n",
      " |      \n",
      " |      >>> df.explain(mode=\"formatted\")\n",
      " |      == Physical Plan ==\n",
      " |      * Scan ExistingRDD (1)\n",
      " |      (1) Scan ExistingRDD [codegen id : 1]\n",
      " |      Output [2]: [age#0, name#1]\n",
      " |      ...\n",
      " |      \n",
      " |      >>> df.explain(\"cost\")\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...Statistics...\n",
      " |      ...\n",
      " |  \n",
      " |  fillna(self, value: Union[ForwardRef('LiteralType'), Dict[str, ForwardRef('LiteralType')]], subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame'\n",
      " |      Replace null values, alias for ``na.fill()``.\n",
      " |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : int, float, string, bool or dict\n",
      " |          Value to replace null values with.\n",
      " |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      " |          from column name (string) to replacement value. The replacement value must be\n",
      " |          an int, float, boolean, or string.\n",
      " |      subset : str, tuple or list, optional\n",
      " |          optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df4.na.fill(50).show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      |  5|    50|  Bob|\n",
      " |      | 50|    50|  Tom|\n",
      " |      | 50|    50| null|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      >>> df5.na.fill(False).show()\n",
      " |      +----+-------+-----+\n",
      " |      | age|   name|  spy|\n",
      " |      +----+-------+-----+\n",
      " |      |  10|  Alice|false|\n",
      " |      |   5|    Bob|false|\n",
      " |      |null|Mallory| true|\n",
      " |      +----+-------+-----+\n",
      " |      \n",
      " |      >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      " |      +---+------+-------+\n",
      " |      |age|height|   name|\n",
      " |      +---+------+-------+\n",
      " |      | 10|    80|  Alice|\n",
      " |      |  5|  null|    Bob|\n",
      " |      | 50|  null|    Tom|\n",
      " |      | 50|  null|unknown|\n",
      " |      +---+------+-------+\n",
      " |  \n",
      " |  filter(self, condition: 'ColumnOrName') -> 'DataFrame'\n",
      " |      Filters rows using the given condition.\n",
      " |      \n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      condition : :class:`Column` or str\n",
      " |          a :class:`Column` of :class:`types.BooleanType`\n",
      " |          or a string of SQL expression.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.age > 3).collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df.where(df.age == 2).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      >>> df.filter(\"age > 3\").collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df.where(\"age = 2\").collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  first(self) -> Optional[pyspark.sql.types.Row]\n",
      " |      Returns the first row as a :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.first()\n",
      " |      Row(age=2, name='Alice')\n",
      " |  \n",
      " |  foreach(self, f: Callable[[pyspark.sql.types.Row], NoneType]) -> None\n",
      " |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a shorthand for ``df.rdd.foreach()``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> def f(person):\n",
      " |      ...     print(person.name)\n",
      " |      >>> df.foreach(f)\n",
      " |  \n",
      " |  foreachPartition(self, f: Callable[[Iterator[pyspark.sql.types.Row]], NoneType]) -> None\n",
      " |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This a shorthand for ``df.rdd.foreachPartition()``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> def f(people):\n",
      " |      ...     for person in people:\n",
      " |      ...         print(person.name)\n",
      " |      >>> df.foreachPartition(f)\n",
      " |  \n",
      " |  freqItems(self, cols: Union[List[str], Tuple[str]], support: Optional[float] = None) -> 'DataFrame'\n",
      " |      Finding frequent items for columns, possibly with false positives. Using the\n",
      " |      frequent element count algorithm described in\n",
      " |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      " |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : list or tuple\n",
      " |          Names of the columns to calculate frequent items for as a list or tuple of\n",
      " |          strings.\n",
      " |      support : float, optional\n",
      " |          The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      " |          The support must be greater than 1e-4.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function is meant for exploratory data analysis, as we make no\n",
      " |      guarantee about the backward compatibility of the schema of the resulting\n",
      " |      :class:`DataFrame`.\n",
      " |  \n",
      " |  groupBy(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
      " |      Groups the :class:`DataFrame` using the specified columns,\n",
      " |      so we can run aggregation on them. See :class:`GroupedData`\n",
      " |      for all the available aggregate functions.\n",
      " |      \n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : list, str or :class:`Column`\n",
      " |          columns to group by.\n",
      " |          Each element should be a column name (string) or an expression (:class:`Column`).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.groupBy().avg().collect()\n",
      " |      [Row(avg(age)=3.5)]\n",
      " |      >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
      " |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      " |      >>> sorted(df.groupBy(df.name).avg().collect())\n",
      " |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      " |      >>> sorted(df.groupBy(['name', df.age]).count().collect())\n",
      " |      [Row(name='Alice', age=2, count=1), Row(name='Bob', age=5, count=1)]\n",
      " |  \n",
      " |  groupby = groupBy(self, *cols)\n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  head(self, n: Optional[int] = None) -> Union[pyspark.sql.types.Row, NoneType, List[pyspark.sql.types.Row]]\n",
      " |      Returns the first ``n`` rows.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int, optional\n",
      " |          default 1. Number of rows to return.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      If n is greater than 1, return a list of :class:`Row`.\n",
      " |      If n is 1, return a single Row.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.head()\n",
      " |      Row(age=2, name='Alice')\n",
      " |      >>> df.head(1)\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  hint(self, name: str, *parameters: Union[ForwardRef('PrimitiveType'), List[ForwardRef('PrimitiveType')]]) -> 'DataFrame'\n",
      " |      Specifies some hint on the current :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          A name of the hint.\n",
      " |      parameters : str, list, float or int\n",
      " |          Optional parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.join(df2.hint(\"broadcast\"), \"name\").show()\n",
      " |      +----+---+------+\n",
      " |      |name|age|height|\n",
      " |      +----+---+------+\n",
      " |      | Bob|  5|    85|\n",
      " |      +----+---+------+\n",
      " |  \n",
      " |  inputFiles(self) -> List[str]\n",
      " |      Returns a best-effort snapshot of the files that compose this :class:`DataFrame`.\n",
      " |      This method simply asks each constituent BaseRelation for its respective files and\n",
      " |      takes the union of all results. Depending on the source relations, this may not find\n",
      " |      all input files. Duplicates are removed.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.load(\"examples/src/main/resources/people.json\", format=\"json\")\n",
      " |      >>> len(df.inputFiles())\n",
      " |      1\n",
      " |  \n",
      " |  intersect(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing rows only in\n",
      " |      both this :class:`DataFrame` and another :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT` in SQL.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  intersectAll(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing rows in both this :class:`DataFrame`\n",
      " |      and another :class:`DataFrame` while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT ALL` in SQL. As standard in SQL, this function\n",
      " |      resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      \n",
      " |      >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  b|  3|\n",
      " |      +---+---+\n",
      " |  \n",
      " |  isEmpty(self) -> bool\n",
      " |      Returns ``True`` if this :class:`DataFrame` is empty.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df_empty = spark.createDataFrame([], 'a STRING')\n",
      " |      >>> df_non_empty = spark.createDataFrame([(\"a\")], 'STRING')\n",
      " |      >>> df_empty.isEmpty()\n",
      " |      True\n",
      " |      >>> df_non_empty.isEmpty()\n",
      " |      False\n",
      " |  \n",
      " |  isLocal(self) -> bool\n",
      " |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
      " |      (without any Spark executors).\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  join(self, other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame'\n",
      " |      Joins with another :class:`DataFrame`, using the given join expression.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Right side of the join\n",
      " |      on : str, list or :class:`Column`, optional\n",
      " |          a string for the join column name, a list of column names,\n",
      " |          a join expression (Column), or a list of Columns.\n",
      " |          If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      " |          the column(s) must exist on both sides, and this performs an equi-join.\n",
      " |      how : str, optional\n",
      " |          default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      " |          ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      " |          ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      " |          ``anti``, ``leftanti`` and ``left_anti``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      The following performs a full outer join between ``df1`` and ``df2``.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import desc\n",
      " |      >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height)                 .sort(desc(\"name\")).collect()\n",
      " |      [Row(name='Bob', height=85), Row(name='Alice', height=None), Row(name=None, height=80)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).collect()\n",
      " |      [Row(name='Tom', height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      " |      \n",
      " |      >>> cond = [df.name == df3.name, df.age == df3.age]\n",
      " |      >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n",
      " |      [Row(name='Bob', height=85)]\n",
      " |      \n",
      " |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n",
      " |      [Row(name='Bob', age=5)]\n",
      " |  \n",
      " |  limit(self, num: int) -> 'DataFrame'\n",
      " |      Limits the result count to the number specified.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.limit(1).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.limit(0).collect()\n",
      " |      []\n",
      " |  \n",
      " |  localCheckpoint(self, eager: bool = True) -> 'DataFrame'\n",
      " |      Returns a locally checkpointed version of this :class:`DataFrame`. Checkpointing can be\n",
      " |      used to truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      " |      iterative algorithms where the plan may grow exponentially. Local checkpoints are\n",
      " |      stored in the executors using the caching subsystem and therefore they are not reliable.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eager : bool, optional\n",
      " |          Whether to checkpoint this :class:`DataFrame` immediately\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental.\n",
      " |  \n",
      " |  observe(self, observation: 'Observation', *exprs: pyspark.sql.column.Column) -> 'DataFrame'\n",
      " |      Observe (named) metrics through an :class:`Observation` instance.\n",
      " |      \n",
      " |      A user can retrieve the metrics by accessing `Observation.get`.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      observation : :class:`Observation`\n",
      " |          an :class:`Observation` instance to obtain the metric.\n",
      " |      exprs : list of :class:`Column`\n",
      " |          column expressions (:class:`Column`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          the observed :class:`DataFrame`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method does not support streaming datasets.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import col, count, lit, max\n",
      " |      >>> from pyspark.sql import Observation\n",
      " |      >>> observation = Observation(\"my metrics\")\n",
      " |      >>> observed_df = df.observe(observation, count(lit(1)).alias(\"count\"), max(col(\"age\")))\n",
      " |      >>> observed_df.count()\n",
      " |      2\n",
      " |      >>> observation.get\n",
      " |      {'count': 2, 'max(age)': 5}\n",
      " |      \n",
      " |      .. versionadded:: 3.3\n",
      " |  \n",
      " |  orderBy = sort(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
      " |  \n",
      " |  pandas_api(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
      " |      Converts the existing DataFrame into a pandas-on-Spark DataFrame.\n",
      " |      \n",
      " |      If a pandas-on-Spark DataFrame is converted to a Spark DataFrame and then back\n",
      " |      to pandas-on-Spark, it will lose the index information and the original index\n",
      " |      will be turned into a normal column.\n",
      " |      \n",
      " |      This is only available if Pandas is installed and available.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      index_col: str or list of str, optional, default: None\n",
      " |          Index column of table in Spark.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.pandas.frame.DataFrame.to_spark\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.show()  # doctest: +SKIP\n",
      " |      +----+----+\n",
      " |      |Col1|Col2|\n",
      " |      +----+----+\n",
      " |      |   a|   1|\n",
      " |      |   b|   2|\n",
      " |      |   c|   3|\n",
      " |      +----+----+\n",
      " |      \n",
      " |      >>> df.pandas_api()  # doctest: +SKIP\n",
      " |        Col1  Col2\n",
      " |      0    a     1\n",
      " |      1    b     2\n",
      " |      2    c     3\n",
      " |      \n",
      " |      We can specify the index columns.\n",
      " |      \n",
      " |      >>> df.pandas_api(index_col=\"Col1\"): # doctest: +SKIP\n",
      " |            Col2\n",
      " |      Col1\n",
      " |      a        1\n",
      " |      b        2\n",
      " |      c        3\n",
      " |  \n",
      " |  persist(self, storageLevel: pyspark.storagelevel.StorageLevel = StorageLevel(True, True, False, True, 1)) -> 'DataFrame'\n",
      " |      Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
      " |      operations after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (`MEMORY_AND_DISK_DESER`)\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
      " |  \n",
      " |  printSchema(self) -> None\n",
      " |      Prints out the schema in the tree format.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.printSchema()\n",
      " |      root\n",
      " |       |-- age: integer (nullable = true)\n",
      " |       |-- name: string (nullable = true)\n",
      " |      <BLANKLINE>\n",
      " |  \n",
      " |  randomSplit(self, weights: List[float], seed: Optional[int] = None) -> List[ForwardRef('DataFrame')]\n",
      " |      Randomly splits this :class:`DataFrame` with the provided weights.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      weights : list\n",
      " |          list of doubles as weights with which to split the :class:`DataFrame`.\n",
      " |          Weights will be normalized if they don't sum up to 1.0.\n",
      " |      seed : int, optional\n",
      " |          The seed for sampling.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> splits = df4.randomSplit([1.0, 2.0], 24)\n",
      " |      >>> splits[0].count()\n",
      " |      2\n",
      " |      \n",
      " |      >>> splits[1].count()\n",
      " |      2\n",
      " |  \n",
      " |  registerTempTable(self, name: str) -> None\n",
      " |      Registers this :class:`DataFrame` as a temporary table using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. deprecated:: 2.0.0\n",
      " |          Use :meth:`DataFrame.createOrReplaceTempView` instead.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.registerTempTable(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      True\n",
      " |  \n",
      " |  repartition(self, numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is hash partitioned.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      cols : str or :class:`Column`\n",
      " |          partitioning columns.\n",
      " |      \n",
      " |          .. versionchanged:: 1.6\n",
      " |             Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
      " |             optional if partitioning columns are specified.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.repartition(10).rdd.getNumPartitions()\n",
      " |      10\n",
      " |      >>> data = df.union(df).repartition(\"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> data = data.repartition(7, \"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> data.rdd.getNumPartitions()\n",
      " |      7\n",
      " |      >>> data = data.repartition(3, \"name\", \"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |  \n",
      " |  repartitionByRange(self, numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is range partitioned.\n",
      " |      \n",
      " |      At least one partition-by expression must be specified.\n",
      " |      When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      cols : str or :class:`Column`\n",
      " |          partitioning columns.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Due to performance reasons this method uses sampling to estimate the ranges.\n",
      " |      Hence, the output may not be consistent, since sampling can return different values.\n",
      " |      The sample size can be controlled by the config\n",
      " |      `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
      " |      2\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> df.repartitionByRange(1, \"age\").rdd.getNumPartitions()\n",
      " |      1\n",
      " |      >>> data = df.repartitionByRange(\"age\")\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |  \n",
      " |  replace(self, to_replace: Union[ForwardRef('LiteralType'), List[ForwardRef('LiteralType')], Dict[ForwardRef('LiteralType'), ForwardRef('OptionalPrimitiveType')]], value: Union[ForwardRef('OptionalPrimitiveType'), List[ForwardRef('OptionalPrimitiveType')], pyspark._globals._NoValueType, NoneType] = <no value>, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      " |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      " |      aliases of each other.\n",
      " |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      " |      or strings. Value can have None. When replacing, the new value will be cast\n",
      " |      to the type of the existing column.\n",
      " |      For numeric replacements all values to be replaced should have unique\n",
      " |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      " |      and arbitrary replacement will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      to_replace : bool, int, float, string, list or dict\n",
      " |          Value to be replaced.\n",
      " |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      " |          must be a mapping between a value and a replacement.\n",
      " |      value : bool, int, float, string or None, optional\n",
      " |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
      " |          list, `value` should be of the same length and type as `to_replace`.\n",
      " |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      " |          used as a replacement for each item in `to_replace`.\n",
      " |      subset : list, optional\n",
      " |          optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df4.na.replace(10, 20).show()\n",
      " |      +----+------+-----+\n",
      " |      | age|height| name|\n",
      " |      +----+------+-----+\n",
      " |      |  20|    80|Alice|\n",
      " |      |   5|  null|  Bob|\n",
      " |      |null|  null|  Tom|\n",
      " |      |null|  null| null|\n",
      " |      +----+------+-----+\n",
      " |      \n",
      " |      >>> df4.na.replace('Alice', None).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace({'Alice': None}).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|   A|\n",
      " |      |   5|  null|   B|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |  \n",
      " |  rollup(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
      " |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregation on them.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |  \n",
      " |  sameSemantics(self, other: 'DataFrame') -> bool\n",
      " |      Returns `True` when the logical query plans inside both :class:`DataFrame`\\s are equal and\n",
      " |      therefore return same results.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The equality comparison here is simplified by tolerating the cosmetic differences\n",
      " |      such as attribute names.\n",
      " |      \n",
      " |      This API can compare both :class:`DataFrame`\\s very fast but can still return\n",
      " |      `False` on the :class:`DataFrame` that return the same results, for instance, from\n",
      " |      different plans. Such false negative semantic can be useful when caching as an example.\n",
      " |      \n",
      " |      This API is a developer API.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.range(10)\n",
      " |      >>> df2 = spark.range(10)\n",
      " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))\n",
      " |      True\n",
      " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id + 2))\n",
      " |      False\n",
      " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\n",
      " |      True\n",
      " |  \n",
      " |  sample(self, withReplacement: Union[float, bool, NoneType] = None, fraction: Union[int, float, NoneType] = None, seed: Optional[int] = None) -> 'DataFrame'\n",
      " |      Returns a sampled subset of this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      withReplacement : bool, optional\n",
      " |          Sample with replacement or not (default ``False``).\n",
      " |      fraction : float, optional\n",
      " |          Fraction of rows to generate, range [0.0, 1.0].\n",
      " |      seed : int, optional\n",
      " |          Seed for sampling (default a random seed).\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |      count of the given :class:`DataFrame`.\n",
      " |      \n",
      " |      `fraction` is required and, `withReplacement` and `seed` are optional.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(10)\n",
      " |      >>> df.sample(0.5, 3).count()\n",
      " |      7\n",
      " |      >>> df.sample(fraction=0.5, seed=3).count()\n",
      " |      7\n",
      " |      >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n",
      " |      1\n",
      " |      >>> df.sample(1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(fraction=1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(False, fraction=1.0).count()\n",
      " |      10\n",
      " |  \n",
      " |  sampleBy(self, col: 'ColumnOrName', fractions: Dict[Any, float], seed: Optional[int] = None) -> 'DataFrame'\n",
      " |      Returns a stratified sample without replacement based on the\n",
      " |      fraction given on each stratum.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col : :class:`Column` or str\n",
      " |          column that defines strata\n",
      " |      \n",
      " |          .. versionchanged:: 3.0\n",
      " |             Added sampling by a column of :class:`Column`\n",
      " |      fractions : dict\n",
      " |          sampling fraction for each stratum. If a stratum is not\n",
      " |          specified, we treat its fraction as zero.\n",
      " |      seed : int, optional\n",
      " |          random seed\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a new :class:`DataFrame` that represents the stratified sample\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      " |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      " |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      " |      +---+-----+\n",
      " |      |key|count|\n",
      " |      +---+-----+\n",
      " |      |  0|    3|\n",
      " |      |  1|    6|\n",
      " |      +---+-----+\n",
      " |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
      " |      33\n",
      " |  \n",
      " |  select(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
      " |      Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, :class:`Column`, or list\n",
      " |          column names (string) or expressions (:class:`Column`).\n",
      " |          If one of the column names is '*', that column is expanded to include all columns\n",
      " |          in the current :class:`DataFrame`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select('*').collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df.select('name', 'age').collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      >>> df.select(df.name, (df.age + 10).alias('age')).collect()\n",
      " |      [Row(name='Alice', age=12), Row(name='Bob', age=15)]\n",
      " |  \n",
      " |  selectExpr(self, *expr: Union[str, List[str]]) -> 'DataFrame'\n",
      " |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a variant of :func:`select` that accepts SQL expressions.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n",
      " |      [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n",
      " |  \n",
      " |  semanticHash(self) -> int\n",
      " |      Returns a hash code of the logical query plan against this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Unlike the standard hash code, the hash is calculated against the query plan\n",
      " |      simplified by tolerating the cosmetic differences such as attribute names.\n",
      " |      \n",
      " |      This API is a developer API.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.range(10).selectExpr(\"id as col0\").semanticHash()  # doctest: +SKIP\n",
      " |      1855039936\n",
      " |      >>> spark.range(10).selectExpr(\"id as col1\").semanticHash()  # doctest: +SKIP\n",
      " |      1855039936\n",
      " |  \n",
      " |  show(self, n: int = 20, truncate: Union[bool, int] = True, vertical: bool = False) -> None\n",
      " |      Prints the first ``n`` rows to the console.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int, optional\n",
      " |          Number of rows to show.\n",
      " |      truncate : bool or int, optional\n",
      " |          If set to ``True``, truncate strings longer than 20 chars by default.\n",
      " |          If set to a number greater than one, truncates long strings to length ``truncate``\n",
      " |          and align cells right.\n",
      " |      vertical : bool, optional\n",
      " |          If set to ``True``, print output rows vertically (one line\n",
      " |          per column value).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df\n",
      " |      DataFrame[age: int, name: string]\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> df.show(truncate=3)\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |  2| Ali|\n",
      " |      |  5| Bob|\n",
      " |      +---+----+\n",
      " |      >>> df.show(vertical=True)\n",
      " |      -RECORD 0-----\n",
      " |       age  | 2\n",
      " |       name | Alice\n",
      " |      -RECORD 1-----\n",
      " |       age  | 5\n",
      " |       name | Bob\n",
      " |  \n",
      " |  sort(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, list, or :class:`Column`, optional\n",
      " |           list of :class:`Column` or column names to sort by.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      ascending : bool or list, optional\n",
      " |          boolean or list of boolean (default ``True``).\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, length of the list must equal length of the `cols`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.sort(df.age.desc()).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.sort(\"age\", ascending=False).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.orderBy(df.age.desc()).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> from pyspark.sql.functions import *\n",
      " |      >>> df.sort(asc(\"age\")).collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df.orderBy(desc(\"age\"), \"name\").collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  sortWithinPartitions(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, list or :class:`Column`, optional\n",
      " |          list of :class:`Column` or column names to sort by.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      ascending : bool or list, optional\n",
      " |          boolean or list of boolean (default ``True``).\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, length of the list must equal length of the `cols`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.sortWithinPartitions(\"age\", ascending=False).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |  \n",
      " |  subtract(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
      " |      but not in another :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  summary(self, *statistics: str) -> 'DataFrame'\n",
      " |      Computes specified statistics for numeric and string columns. Available statistics are:\n",
      " |      - count\n",
      " |      - mean\n",
      " |      - stddev\n",
      " |      - min\n",
      " |      - max\n",
      " |      - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n",
      " |      \n",
      " |      If no statistics are given, this function computes count, mean, stddev, min,\n",
      " |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function is meant for exploratory data analysis, as we make no\n",
      " |      guarantee about the backward compatibility of the schema of the resulting\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
      " |      ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
      " |      ... )\n",
      " |      >>> df.select(\"age\", \"weight\", \"height\").summary().show()\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |      |summary| age|            weight|           height|\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |      |  count|   3|                 3|                3|\n",
      " |      |   mean|12.0| 40.73333333333333|            145.0|\n",
      " |      | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
      " |      |    min|  11|              37.8|            142.2|\n",
      " |      |    25%|  11|              37.8|            142.2|\n",
      " |      |    50%|  12|              40.3|            142.3|\n",
      " |      |    75%|  13|              44.1|            150.5|\n",
      " |      |    max|  13|              44.1|            150.5|\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |      \n",
      " |      >>> df.select(\"age\", \"weight\", \"height\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
      " |      +-------+---+------+------+\n",
      " |      |summary|age|weight|height|\n",
      " |      +-------+---+------+------+\n",
      " |      |  count|  3|     3|     3|\n",
      " |      |    min| 11|  37.8| 142.2|\n",
      " |      |    25%| 11|  37.8| 142.2|\n",
      " |      |    75%| 13|  44.1| 150.5|\n",
      " |      |    max| 13|  44.1| 150.5|\n",
      " |      +-------+---+------+------+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.display\n",
      " |  \n",
      " |  tail(self, num: int) -> List[pyspark.sql.types.Row]\n",
      " |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |      \n",
      " |      Running tail requires moving data into the application's driver process, and doing so with\n",
      " |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.tail(1)\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  take(self, num: int) -> List[pyspark.sql.types.Row]\n",
      " |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.take(2)\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  toDF(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` that with new specified column names\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str\n",
      " |          new column names\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.toDF('f1', 'f2').collect()\n",
      " |      [Row(f1=2, f2='Alice'), Row(f1=5, f2='Bob')]\n",
      " |  \n",
      " |  toJSON(self, use_unicode: bool = True) -> pyspark.rdd.RDD[str]\n",
      " |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
      " |      \n",
      " |      Each row is turned into a JSON document as one element in the returned RDD.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.toJSON().first()\n",
      " |      '{\"age\":2,\"name\":\"Alice\"}'\n",
      " |  \n",
      " |  toLocalIterator(self, prefetchPartitions: bool = False) -> Iterator[pyspark.sql.types.Row]\n",
      " |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
      " |      The iterator will consume as much memory as the largest partition in this\n",
      " |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n",
      " |      partitions.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      prefetchPartitions : bool, optional\n",
      " |          If Spark should pre-fetch the next partition  before it is needed.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> list(df.toLocalIterator())\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  to_koalas(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
      " |      # Keep to_koalas for backward compatibility for now.\n",
      " |  \n",
      " |  to_pandas_on_spark(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
      " |      # Keep to_pandas_on_spark for backward compatibility for now.\n",
      " |  \n",
      " |  transform(self, func: Callable[..., ForwardRef('DataFrame')], *args: Any, **kwargs: Any) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          a function that takes and returns a :class:`DataFrame`.\n",
      " |      *args\n",
      " |          Positional arguments to pass to func.\n",
      " |      \n",
      " |          .. versionadded:: 3.3.0\n",
      " |      **kwargs\n",
      " |          Keyword arguments to pass to func.\n",
      " |      \n",
      " |          .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
      " |      >>> def cast_all_to_int(input_df):\n",
      " |      ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
      " |      >>> def sort_columns_asc(input_df):\n",
      " |      ...     return input_df.select(*sorted(input_df.columns))\n",
      " |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
      " |      +-----+---+\n",
      " |      |float|int|\n",
      " |      +-----+---+\n",
      " |      |    1|  1|\n",
      " |      |    2|  2|\n",
      " |      +-----+---+\n",
      " |      >>> def add_n(input_df, n):\n",
      " |      ...     return input_df.select([(col(col_name) + n).alias(col_name)\n",
      " |      ...                             for col_name in input_df.columns])\n",
      " |      >>> df.transform(add_n, 1).transform(add_n, n=10).show()\n",
      " |      +---+-----+\n",
      " |      |int|float|\n",
      " |      +---+-----+\n",
      " |      | 12| 12.0|\n",
      " |      | 13| 13.0|\n",
      " |      +---+-----+\n",
      " |  \n",
      " |  union(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  unionAll(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  unionByName(self, other: 'DataFrame', allowMissingColumns: bool = False) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n",
      " |      union (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      The difference between this function and :func:`union` is that this function\n",
      " |      resolves columns by name (not by position):\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      " |      >>> df1.unionByName(df2).show()\n",
      " |      +----+----+----+\n",
      " |      |col0|col1|col2|\n",
      " |      +----+----+----+\n",
      " |      |   1|   2|   3|\n",
      " |      |   6|   4|   5|\n",
      " |      +----+----+----+\n",
      " |      \n",
      " |      When the parameter `allowMissingColumns` is ``True``, the set of column names\n",
      " |      in this and other :class:`DataFrame` can differ; missing columns will be filled with null.\n",
      " |      Further, the missing columns of this :class:`DataFrame` will be added at the end\n",
      " |      in the schema of the union result:\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n",
      " |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
      " |      +----+----+----+----+\n",
      " |      |col0|col1|col2|col3|\n",
      " |      +----+----+----+----+\n",
      " |      |   1|   2|   3|null|\n",
      " |      |null|   4|   5|   6|\n",
      " |      +----+----+----+----+\n",
      " |      \n",
      " |      .. versionchanged:: 3.1.0\n",
      " |         Added optional argument `allowMissingColumns` to specify whether to allow\n",
      " |         missing columns.\n",
      " |  \n",
      " |  unpersist(self, blocking: bool = False) -> 'DataFrame'\n",
      " |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
      " |  \n",
      " |  where = filter(self, condition)\n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withColumn(self, colName: str, col: pyspark.sql.column.Column) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      " |      existing column that has the same name.\n",
      " |      \n",
      " |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      " |      a column from some other :class:`DataFrame` will raise an error.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      colName : str\n",
      " |          string, name of the new column.\n",
      " |      col : :class:`Column`\n",
      " |          a :class:`Column` expression for the new column.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method introduces a projection internally. Therefore, calling it multiple\n",
      " |      times, for instance, via loops in order to add multiple columns can generate big\n",
      " |      plans which can cause performance issues and even `StackOverflowException`.\n",
      " |      To avoid this, use :func:`select` with the multiple columns at once.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.withColumn('age2', df.age + 2).collect()\n",
      " |      [Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
      " |  \n",
      " |  withColumnRenamed(self, existing: str, new: str) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` by renaming an existing column.\n",
      " |      This is a no-op if schema doesn't contain the given column name.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      existing : str\n",
      " |          string, name of the existing column to rename.\n",
      " |      new : str\n",
      " |          string, new name of the column.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.withColumnRenamed('age', 'age2').collect()\n",
      " |      [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\n",
      " |  \n",
      " |  withColumns(self, *colsMap: Dict[str, pyspark.sql.column.Column]) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` by adding multiple columns or replacing the\n",
      " |      existing columns that has the same names.\n",
      " |      \n",
      " |      The colsMap is a map of column name and column, the column must only refer to attributes\n",
      " |      supplied by this Dataset. It is an error to add columns that refer to some other Dataset.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |         Added support for multiple columns adding\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      colsMap : dict\n",
      " |          a dict of column name and :class:`Column`. Currently, only single map is supported.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.withColumns({'age2': df.age + 2, 'age3': df.age + 3}).collect()\n",
      " |      [Row(age=2, name='Alice', age2=4, age3=5), Row(age=5, name='Bob', age2=7, age3=8)]\n",
      " |  \n",
      " |  withMetadata(self, columnName: str, metadata: Dict[str, Any]) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` by updating an existing column with metadata.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      columnName : str\n",
      " |          string, name of the existing column to update the metadata.\n",
      " |      metadata : dict\n",
      " |          dict, new metadata to be assigned to df.schema[columnName].metadata\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df_meta = df.withMetadata('age', {'foo': 'bar'})\n",
      " |      >>> df_meta.schema['age'].metadata\n",
      " |      {'foo': 'bar'}\n",
      " |  \n",
      " |  withWatermark(self, eventTime: str, delayThreshold: str) -> 'DataFrame'\n",
      " |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
      " |      in time before which we assume no more late data is going to arrive.\n",
      " |      \n",
      " |      Spark will use this watermark for several purposes:\n",
      " |        - To know when a given time window aggregation can be finalized and thus can be emitted\n",
      " |          when using output modes that do not allow updates.\n",
      " |      \n",
      " |        - To minimize the amount of state that we need to keep for on-going aggregations.\n",
      " |      \n",
      " |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
      " |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
      " |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
      " |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
      " |      process records that arrive more than `delayThreshold` late.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eventTime : str\n",
      " |          the name of the column that contains the event time of the row.\n",
      " |      delayThreshold : str\n",
      " |          the minimum delay to wait to data to arrive late, relative to the\n",
      " |          latest record that has been processed in the form of an interval\n",
      " |          (e.g. \"1 minute\" or \"5 hours\").\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import timestamp_seconds\n",
      " |      >>> sdf.select(\n",
      " |      ...    'name',\n",
      " |      ...    timestamp_seconds(sdf.time).alias('time')).withWatermark('time', '10 minutes')\n",
      " |      DataFrame[name: string, time: timestamp]\n",
      " |  \n",
      " |  writeTo(self, table: str) -> pyspark.sql.readwriter.DataFrameWriterV2\n",
      " |      Create a write configuration builder for v2 sources.\n",
      " |      \n",
      " |      This builder is used to configure and execute write operations.\n",
      " |      \n",
      " |      For example, to append or create or replace existing tables.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.writeTo(\"catalog.db.table\").append()  # doctest: +SKIP\n",
      " |      >>> df.writeTo(                              # doctest: +SKIP\n",
      " |      ...     \"catalog.db.table\"\n",
      " |      ... ).partitionedBy(\"col\").createOrReplace()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  columns\n",
      " |      Returns all column names as a list.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.columns\n",
      " |      ['age', 'name']\n",
      " |  \n",
      " |  dtypes\n",
      " |      Returns all column names and their data types as a list.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'int'), ('name', 'string')]\n",
      " |  \n",
      " |  isStreaming\n",
      " |      Returns ``True`` if this :class:`DataFrame` contains one or more sources that\n",
      " |      continuously return data as it arrives. A :class:`DataFrame` that reads data from a\n",
      " |      streaming source must be executed as a :class:`StreamingQuery` using the :func:`start`\n",
      " |      method in :class:`DataStreamWriter`.  Methods that return a single answer, (e.g.,\n",
      " |      :func:`count` or :func:`collect`) will throw an :class:`AnalysisException` when there\n",
      " |      is a streaming source present.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |  \n",
      " |  na\n",
      " |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  rdd\n",
      " |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  schema\n",
      " |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.schema\n",
      " |      StructType([StructField('age', IntegerType(), True),\n",
      " |                  StructField('name', StringType(), True)])\n",
      " |  \n",
      " |  sparkSession\n",
      " |      Returns Spark session that created this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(1)\n",
      " |      >>> type(df.sparkSession)\n",
      " |      <class 'pyspark.sql.session.SparkSession'>\n",
      " |  \n",
      " |  sql_ctx\n",
      " |  \n",
      " |  stat\n",
      " |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  storageLevel\n",
      " |      Get the :class:`DataFrame`'s current storage level.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.storageLevel\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> df.cache().storageLevel\n",
      " |      StorageLevel(True, True, False, True, 1)\n",
      " |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
      " |      StorageLevel(True, False, False, False, 2)\n",
      " |  \n",
      " |  write\n",
      " |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrameWriter`\n",
      " |  \n",
      " |  writeStream\n",
      " |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataStreamWriter`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      " |  \n",
      " |  mapInArrow(self, func: 'ArrowMapIterFunction', schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrame'\n",
      " |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      " |      function that takes and outputs a PyArrow's `RecordBatch`, and returns the result as a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The function should take an iterator of `pyarrow.RecordBatch`\\s and return\n",
      " |      another iterator of `pyarrow.RecordBatch`\\s. All columns are passed\n",
      " |      together as an iterator of `pyarrow.RecordBatch`\\s to the function and the\n",
      " |      returned iterator of `pyarrow.RecordBatch`\\s are combined as a :class:`DataFrame`.\n",
      " |      Each `pyarrow.RecordBatch` size can be controlled by\n",
      " |      `spark.sql.execution.arrow.maxRecordsPerBatch`.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          a Python native function that takes an iterator of `pyarrow.RecordBatch`\\s, and\n",
      " |          outputs an iterator of `pyarrow.RecordBatch`\\s.\n",
      " |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      " |          the return type of the `func` in PySpark. The value can be either a\n",
      " |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import pyarrow  # doctest: +SKIP\n",
      " |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      " |      >>> def filter_func(iterator):\n",
      " |      ...     for batch in iterator:\n",
      " |      ...         pdf = batch.to_pandas()\n",
      " |      ...         yield pyarrow.RecordBatch.from_pandas(pdf[pdf.id == 1])\n",
      " |      >>> df.mapInArrow(filter_func, df.schema).show()  # doctest: +SKIP\n",
      " |      +---+---+\n",
      " |      | id|age|\n",
      " |      +---+---+\n",
      " |      |  1| 21|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is unstable, and for developers.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.pandas_udf\n",
      " |      pyspark.sql.DataFrame.mapInPandas\n",
      " |  \n",
      " |  mapInPandas(self, func: 'PandasMapIterFunction', schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrame'\n",
      " |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      " |      function that takes and outputs a pandas DataFrame, and returns the result as a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The function should take an iterator of `pandas.DataFrame`\\s and return\n",
      " |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n",
      " |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n",
      " |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n",
      " |      Each `pandas.DataFrame` size can be controlled by\n",
      " |      `spark.sql.execution.arrow.maxRecordsPerBatch`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n",
      " |          outputs an iterator of `pandas.DataFrame`\\s.\n",
      " |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      " |          the return type of the `func` in PySpark. The value can be either a\n",
      " |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import pandas_udf\n",
      " |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      " |      >>> def filter_func(iterator):\n",
      " |      ...     for pdf in iterator:\n",
      " |      ...         yield pdf[pdf.id == 1]\n",
      " |      >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n",
      " |      +---+---+\n",
      " |      | id|age|\n",
      " |      +---+---+\n",
      " |      |  1| 21|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.pandas_udf\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n",
      " |  \n",
      " |  toPandas(self) -> 'PandasDataFrameLike'\n",
      " |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
      " |      \n",
      " |      This is only available if Pandas is installed and available.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting Pandas ``pandas.DataFrame`` is\n",
      " |      expected to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      Usage with ``spark.sql.execution.arrow.pyspark.enabled=True`` is experimental.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.toPandas()  # doctest: +SKIP\n",
      " |         age   name\n",
      " |      0    2  Alice\n",
      " |      1    5    Bob\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df_aluno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc9faa-2be4-4bf6-ae71-5b2bf77a3b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "397a1914-9aeb-4b19-a3b5-bc11e6ce0260",
   "metadata": {},
   "source": [
    "## CRIANDO DF A PARTIR DE ARQUIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be19df8c-7110-4390-ae4a-ca8caf75da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV HADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f2c67-e7e4-43b6-a9e0-04621acd76d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read.csv('/datalake/raw/pessoas',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b770d27-9067-42c7-aaf2-410c7e4c17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSON HADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd03f7-445b-4fe9-a516-beb1f31e846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.json('/datalake/raw/user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9306b8-a002-4835-af4f-359ab0cae9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSON S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18ed9f-13b7-4c54-ae16-4908a1519ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jsons3 = spark.read.json('s3a://camada-bronze/user/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918121e5-be30-491d-bf74-4c880dd97d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste = spark.read.load(\"/datalake/raw/pessoas\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4b5e4-add9-4246-9b3f-93cbfa9124bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV LOCAL\n",
    "df_local = spark.read.csv('file:///home/user/dados.txt',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9802d9-e2d0-4c07-ae2d-a0f9aeb2a42c",
   "metadata": {},
   "source": [
    "## CRIANDO DF A PARTIR DE BANCO DE DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c724c6d-01f5-430a-9a9a-efb01c87fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"jdbc:postgresql://postgres:5432/dvdrental\"\n",
    "\n",
    "properties = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dec4296-0658-4163-8d93-7c319298f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = spark.read.jdbc(url=url,table='public.city',properties=properties)\n",
    "df_county = spark.read.jdbc(url=url,table='public.country',properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37dbddc-2488-48d5-8e05-4cce2d4034bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '(select c.city_id ,city, country from public.city c \\\n",
    "        inner join public.country c2 \\\n",
    "        on c2.country_id  = c.country_id) as tab '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0131911c-1e28-4bc6-94e5-c8140bfe7035",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query = spark.read.jdbc(url=url,table=query,properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d030d75c-d100-4fb8-b1ee-685fc9250c9b",
   "metadata": {},
   "source": [
    "## A√á√ÉO E TRANSFORMA√á√ÉO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2574c10c-6860-455b-9a67-99a267ff9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A√á√ÉO\n",
    "df_city.show(10)\n",
    "df_city.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c2efda-6ca7-45f9-8617-3512036bc2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFORMA√á√ÉO\n",
    "df_city2 = df_city.filter(df_city.country_id == 101).filter(df_city.city.startswith('A'))\n",
    "df_city2.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b513e537-6074-4884-ba63-994c9a586d5a",
   "metadata": {},
   "source": [
    "## VALIDANDO DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450c5d5-663a-4c58-947e-3f6be1e1d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee03f42-8888-431f-8260-6f55dd94449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8979bda-cd9d-4192-9f04-136a1a652139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b70b4-81fd-4b01-aa57-253925c152d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d803a412-9f79-4f28-b6fa-5c1f2c1a5d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba810ae-e862-4d43-93ad-bf180701b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768730f1-9bb2-414d-9de7-2561e093e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20f996a-0426-424b-bfa4-ad590b166222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f2129-4bd8-4038-b7b7-47af0e5cd826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample([withReplacement, fraction, seed])  Returns a sampled subset of this DataFrame.\n",
    "df_city.sample(0.1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f251d6-fdce-42d3-943b-85c407b4195a",
   "metadata": {},
   "source": [
    "## MANIPULANDO DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e2e57-05ef-4853-a119-bf45cc97b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isEmpty() Returns True if this DataFrame is empty.\n",
    "df_city.isEmpty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52cdbf0-2c36-451f-aba4-42eb025966af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select(*cols) Projects a set of expressions and returns a new DataFrame.\n",
    "df_city.select('city').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10fe49-5fcc-4b5a-8bc0-370310545a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#orderBy(*cols, **kwargs) Returns a new DataFrame sorted by the specified column(s).\n",
    "#sort(*cols, **kwargs) Returns a new DataFrame sorted by the specified column(s).\n",
    "\n",
    "df_city.orderBy('city',ascending=True).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8087ff41-62ff-4f27-ad2e-ad2b6761ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "df_city.orderBy(df_city.city.asc()).show(5)\n",
    "df_city.orderBy(df_city.city.desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a5173f-f19e-416f-ab76-d9f97f8fd2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where e filter\n",
    "df_city.where(df_city.city_id == 1).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe58956c-2ef6-42aa-8cf3-f42820188ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.filter(df_city.city_id == 1).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee7306-0c6d-4084-bd1f-8f991020427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fillna(value[, subset]) Replace null values, alias for na.fill().\n",
    "df_city.na.fill('').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a8de87-e1da-4b0f-95fa-66a52cad5456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct() Returns a new DataFrame containing the distinct rows in this DataFrame.\n",
    "df_city.select('city').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0270b0-9983-4f69-a7da-e893f589cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first() Returns the first row as a Row.\n",
    "df_city.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6a2e2-0660-4669-813a-dbcd04ebe9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#head([n]) Returns the first n rows.\n",
    "df_city.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4965716-aa2a-4db7-95f2-279e733d6272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit(num) Limits the result count to the number specified.\n",
    "df_city.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd417001-b706-42b5-8808-d231421d4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tail(num) Returns the last num rows as a list of Row.\n",
    "df_city.tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d7c0c-f459-459c-816f-419173b3b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take(num) Returns the first num rows as a list of Row.\n",
    "df_city.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66737751-c860-4915-bc10-0e4bb2b8d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect() Returns all the records as a list of Row.\n",
    "df_city.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2920e5-17eb-4292-9c87-0b2140e808d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupBy(*cols) Groups the DataFrame using the specified columns, so we can run aggregation on them.\n",
    "df_city.groupby('country_id').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788fb5c-3202-472a-b9e5-854cbb9f8ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg(*exprs) Aggregate on the entire DataFrame without groups (shorthand for df.groupBy().agg()).\n",
    "df_city \\\n",
    "    .groupby('country_id') \\\n",
    "    .agg({\"country_id\": \"sum\"}) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f077bae-668b-4a0c-b3f5-3ba02e2eda00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join inner, cross, outer, full, fullouter, full_outer, left, leftouter, left_outer, right, rightouter, right_outer, semi, leftsemi, left_semi, anti, leftanti and left_anti.\n",
    "df_city.join(df_county,df_city.country_id == df_county.country_id,'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822427ef-7e99-4051-8e16-a3742c4bbb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#union\n",
    "df_city.select('country_id').union(df_county.select('country_id')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d583fdfe-1029-45dc-9317-7856dd3bb055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#toDF(*cols) Returns a new DataFrame that with new specified column names\n",
    "#toJSON([use_unicode])\n",
    "#toPandas()\n",
    "df_city.toJSON()\n",
    "p = df_city.toPandas()\n",
    "p\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac7d318-33cf-496b-a732-1811cfeb62e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#partitions\n",
    "df_city.rdd.getNumPartitions()\n",
    "df_city = df_tab.repartition(2)\n",
    "df_city.rdd.getNumPartitions()\n",
    "#df.rdd.partitions.length()\n",
    "#df.rdd.partitions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddd2060-ffd4-4eaa-ad5f-762142aec0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop(*cols) Returns a new DataFrame without specified columns.\n",
    "df_city.drop('last_update').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038cfe7-db3c-4bcc-a5f6-723842280275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropDuplicates([subset]) Return a new DataFrame with duplicate rows removed, optionally only considering certain columns.\n",
    "df_city.select('city').count()\n",
    "df_city.select('city').dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4730759-2676-45e8-ae29-cf3d5f88f37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropna([how, thresh, subset]) Returns a new DataFrame omitting rows with null values.\n",
    "df_city.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15ea02-c2e7-4607-8167-57d1c448a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#withColumn(colName, col) Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df_city.withColumn('new_id',monotonically_increasing_id()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989435a9-5366-400b-993b-9311bce329c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#withColumnRenamed(existing, new) Returns a new DataFrame by renaming an existing column.\n",
    "df_city.withColumnRenamed('last_update','updated').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf2699-f75b-42f9-ac4f-10ca3e063008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alterar typo de coluna\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df_aluno.withColumn(\"id\",col(\"id\").cast(IntegerType()))\n",
    "df_aluno.withColumn(\"id\",col(\"id\").cast(\"int\"))\n",
    "df_aluno.withColumn(\"id\",col(\"id\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ae1c1-6332-41f3-8cd4-03c6e6a7b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#foreach(f) Applies the f function to all Row of this DataFrame.\n",
    "#foreachPartition(f) Applies the f function to each partition of this DataFrame.\n",
    "def func(df):\n",
    "    print(df.city)\n",
    "df_city.foreach(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e5e14d-7992-4f1f-8b08-92078c67dc16",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7ec67-5ea8-4baa-9503-3c289b82b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#createOrReplaceGlobalTempView(name) \n",
    "#createOrReplaceTempView(name)\n",
    "#createTempView(name)\n",
    "df_city.createOrReplaceTempView('city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce136230-320c-4b6e-b710-e5f836768f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from city limit 5').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01328bf-c8eb-455a-bb8d-9b2fb4a706fb",
   "metadata": {},
   "source": [
    "## PLANO DE EXECU√á√ÉO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0bd8e5-9a11-4483-906a-73cfb6fdc980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explain([extended, mode])Prints the (logical and physical) plans to the console for debugging purposes.\n",
    "df_city.explain() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900aa649-d1a8-446e-8013-e447156482ca",
   "metadata": {},
   "source": [
    "## TUNNING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c74fc8d-b208-44f1-b95a-d98210410921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HABILITADO DESDE O SPARK 3.2\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c236b63a-626c-4e13-9bf9-c0221a96f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache unionAll unpersist\n",
    "df_city.cache()\n",
    "df_city.persist()\n",
    "df_city.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778bcd7c-4d5d-4664-b8d7-5dce07dfff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT\n",
    "spark.sql('SELECT /*+ REPARTITION(5) */ * FROM city;').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c0d83-510a-44b5-a2a3-1e2017379b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BROADCAST VARIABLE\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11f3f22-4d61-4d0b-85fa-e3ad57e20a8a",
   "metadata": {},
   "source": [
    "## SALVANDO DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176b1f6-4b13-4418-adbd-5443323644ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV HADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f85da-0df2-40dd-adb5-4bed6dff6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.write.csv('/datalake/process/city',header=True,sep=',',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec260d-003e-424b-abf6-fba55113e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa5b33-8796-4e6d-b5ff-494fd91aa9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.write.csv('s3a://camada-prata/city/',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d92272-4e02-4df8-92f8-a898bcafd497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e2ad11-d00f-482d-826c-ac4c800bcc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.write.json('s3a://camada-prata/city_json/',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9be38e-bddd-45ec-987f-b06d6a52e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cff875-5c89-47f1-8aea-235326ccc199",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.write.parquet('s3a://camada-prata/city_parquet/',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2541894-b799-4062-9a5f-1c79a99a29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9727a35-43c8-4624-a7b9-879f4777a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM parquet.`s3a://camada-prata/city_parquet/`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b936ae-9eb6-4a70-bc97-cf7db2d7bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268af9d-eee0-44d7-9082-dc3dbc7d15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.write.orc('s3a://camada-prata/city_orc/',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b19c83-c4ae-4de7-b4d1-b3d792728f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATABASE\n",
    "df_city.write \\\n",
    "    .jdbc(url=url, table='public.df',properties=properties,mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e50e66-cb97-4db0-b8d8-eb1a4da1f775",
   "metadata": {},
   "source": [
    "## HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a4426f-2198-4d55-8f67-9f62718ba302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark.read.json('s3a://camada-bronze/user/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940f275-f32e-4be3-943e-f9ad17eeaa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = \\\n",
    "df_raw.select( \\\n",
    "     'address.city' \\\n",
    "    ,'address.coordinates.lat' \\\n",
    "    ,'address.coordinates.lng' \\\n",
    "    ,'address.country' \\\n",
    "    ,'address.state' \\\n",
    "    ,'address.street_address' \\\n",
    "    ,'address.street_name' \\\n",
    "    ,'address.zip_code' \\\n",
    "    ,'avatar' \\\n",
    "    ,'credit_card.cc_number' \\\n",
    "    ,'date_of_birth' \\\n",
    "    ,'email' \\\n",
    "    ,'employment.key_skill' \\\n",
    "    ,'employment.title' \\\n",
    "    ,'first_name' \\\n",
    "    ,'gender' \\\n",
    "    ,'id' \\\n",
    "    ,'last_name' \\\n",
    "    ,'password' \\\n",
    "    ,'phone_number' \\\n",
    "    ,'social_insurance_number' \\\n",
    "    ,'subscription.payment_method' \\\n",
    "    ,'subscription.plan' \\\n",
    "    ,'subscription.status' \\\n",
    "    ,'subscription.term' \\\n",
    "    ,'uid' \\\n",
    "    ,'username' \\\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcddc09c-fdad-41e2-9892-98c56d013c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee88dea8-178c-443a-b0f2-3103c20639f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.write.format('hive').saveAsTable('default.teste3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2962e804-c249-433a-a2bb-09c57fb45943",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables from default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809e89c-9fdd-44cd-8383-a4549959c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARTICIONADO\n",
    "df.write.partitionBy(\"country_id\").format(\"parquet\").save(\"/process/tabpart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20430bee-4d99-4fac-bd61-f06e77b54911",
   "metadata": {},
   "source": [
    "## DELTA LAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a1474e5-4696-48d4-b7d9-bc286f2273ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder\n",
    "    .appName(\"delta\")\n",
    "     .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"datalake\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"datalake\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa17a5f4-7a80-407c-988a-22522e674047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>delta</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7af58c487340>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "998f2573-ec31-4ee0-95d8-8862349298c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|fact                                                                                                                                                                                                                        |length|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|In Japan, cats are thought to have the power to turn into super spirits when they die. This may be because according to the Buddhist religion, the body of the cat is the temporary resting place of very spiritual people.i|220   |\n",
      "|Cats, just like people, are subject to asthma. Dust, smoke, and other forms of air pullution in your cat's environment can be troublesome sources of irritation.                                                            |160   |\n",
      "|It has been scientifically proven that stroking a cat can lower one's blood pressure.                                                                                                                                       |85    |\n",
      "|The domestic cat is the only species able to hold its tail vertically while walking. You can also learn about your cat's present state of mind by observing the posture of his tail.                                        |180   |\n",
      "|Cats do not think that they are little people. They think that we are big cats. This influences their behavior in many ways.                                                                                                |124   |\n",
      "|Cats lap liquid from the underside of their tongue, not from the top.                                                                                                                                                       |69    |\n",
      "|Cats have been domesticated for half as long as dogs have been.                                                                                                                                                             |63    |\n",
      "|Heat occurs several times a year and can last anywhere from 3 to 15 days.                                                                                                                                                   |73    |\n",
      "|Cats have about 130,000 hairs per square inch (20,155 hairs per square centimeter).                                                                                                                                         |83    |\n",
      "|In 1888, more than 300,000 mummified cats were found an Egyptian cemetery. They were stripped of their wrappings and carted off to be used by farmers in England and the U.S. for fertilizer.                               |189   |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def loadData(qtde):\n",
    "    items = []\n",
    "    for _ in range(qtde):\n",
    "        r = requests.get('https://catfact.ninja/fact')\n",
    "        items.append(r.json())\n",
    "\n",
    "    # cria DF diretamente\n",
    "    return spark.createDataFrame(items)\n",
    "\n",
    "df = loadData(10)\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec29260-173b-4d5b-a625-573c6d4cd761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                fact|length|\n",
      "+--------------------+------+\n",
      "|In Japan, cats ar...|   220|\n",
      "|Cats, just like p...|   160|\n",
      "|It has been scien...|    85|\n",
      "|The domestic cat ...|   180|\n",
      "|Cats do not think...|   124|\n",
      "|Cats lap liquid f...|    69|\n",
      "|Cats have been do...|    63|\n",
      "|Heat occurs sever...|    73|\n",
      "|Cats have about 1...|    83|\n",
      "|In 1888, more tha...|   189|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05bfa7b8-80df-4392-b429-170066ee9087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|fact                                                                                                                                                                                                                        |length|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|In Japan, cats are thought to have the power to turn into super spirits when they die. This may be because according to the Buddhist religion, the body of the cat is the temporary resting place of very spiritual people.i|220   |\n",
      "|Cats, just like people, are subject to asthma. Dust, smoke, and other forms of air pullution in your cat's environment can be troublesome sources of irritation.                                                            |160   |\n",
      "|It has been scientifically proven that stroking a cat can lower one's blood pressure.                                                                                                                                       |85    |\n",
      "|The domestic cat is the only species able to hold its tail vertically while walking. You can also learn about your cat's present state of mind by observing the posture of his tail.                                        |180   |\n",
      "|Cats do not think that they are little people. They think that we are big cats. This influences their behavior in many ways.                                                                                                |124   |\n",
      "|Cats lap liquid from the underside of their tongue, not from the top.                                                                                                                                                       |69    |\n",
      "|Cats have been domesticated for half as long as dogs have been.                                                                                                                                                             |63    |\n",
      "|Heat occurs several times a year and can last anywhere from 3 to 15 days.                                                                                                                                                   |73    |\n",
      "|Cats have about 130,000 hairs per square inch (20,155 hairs per square centimeter).                                                                                                                                         |83    |\n",
      "|In 1888, more than 300,000 mummified cats were found an Egyptian cemetery. They were stripped of their wrappings and carted off to be used by farmers in England and the U.S. for fertilizer.                               |189   |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a32201-c258-4ec5-b533-32c007a821a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 's3a://raw/user_app'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd30e9f0-b6ca-41eb-920a-a37618d48c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.driver.memory', '1G'),\n",
       " ('spark.hadoop.fs.s3a.path.style.access', 'true'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/home/user/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar,/home/user/.ivy2/jars/io.delta_delta-storage-2.2.0.jar,/home/user/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar'),\n",
       " ('spark.app.startTime', '1764815535802'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.initial.file.urls',\n",
       "  'file:///home/user/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar,file:///home/user/.ivy2/jars/io.delta_delta-storage-2.2.0.jar,file:///home/user/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '46549'),\n",
       " ('spark.hadoop.fs.s3a.access.key', 'datalake'),\n",
       " ('spark.driver.host', 'spark-master'),\n",
       " ('spark.jars',\n",
       "  'file:///home/user/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar,file:///home/user/.ivy2/jars/io.delta_delta-storage-2.2.0.jar,file:///home/user/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.sql.warehouse.dir', 'file:/home/user/spark-warehouse'),\n",
       " ('spark.app.name', 'delta'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://spark-master:46549/jars/io.delta_delta-core_2.12-2.2.0.jar,spark://spark-master:46549/jars/org.antlr_antlr4-runtime-4.8.jar,spark://spark-master:46549/jars/io.delta_delta-storage-2.2.0.jar'),\n",
       " ('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem'),\n",
       " ('spark.files',\n",
       "  'file:///home/user/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar,file:///home/user/.ivy2/jars/io.delta_delta-storage-2.2.0.jar,file:///home/user/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar'),\n",
       " ('spark.jars.packages', 'io.delta:delta-core_2.12:2.2.0'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.hadoop.fs.s3a.endpoint', 'http://minio:9000'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///home/user/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar,file:///home/user/.ivy2/jars/io.delta_delta-storage-2.2.0.jar,file:///home/user/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar'),\n",
       " ('spark.app.id', 'local-1764815536528'),\n",
       " ('spark.app.submitTime', '1764815535593'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.hadoop.fs.s3a.secret.key', 'datalake'),\n",
       " ('spark.sql.catalog.spark_catalog',\n",
       "  'org.apache.spark.sql.delta.catalog.DeltaCatalog')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "353d05bf-cfa1-4167-bea9-71daeb9df102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvando como delta table\n",
    "df.write.format(\"delta\").mode('overwrite').save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3554242-7b9f-4b1e-9bf8-055f51eb8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5711fd48-211f-48b7-b25e-0e7371eb8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(deltaTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124939f9-3cc3-4ea8-82e7-c4e8ec7ebde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = spark.read.format(\"delta\").load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fc1e25-47e9-4427-a88d-6c1d68d754f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54428c-00b3-454a-93f6-2d4455027c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.toDF().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb8e2ca-b694-4fa3-ac53-a3395bf9d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MERGE\n",
    "#NOVOS DADOS\n",
    "raw = loadData(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c24362-391b-4edd-8ac5-e7a98651f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a07abb-27a4-4e86-8874-af6feb0bb435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "deltaTable.alias(\"process\") \\\n",
    "  .merge(\n",
    "    raw.alias(\"raw\"),\n",
    "    \"process.id = raw.id\") \\\n",
    "  .whenNotMatchedInsert(values = {'email' : col('email') \\\n",
    "                                 ,'first_name' : col('first_name') \\\n",
    "                                 ,'last_name' : col('last_name') \\\n",
    "                                 ,'gender' : col('gender') \\\n",
    "                                 ,'id' : col('id') \\\n",
    "                                 ,'username' : col('username') \\\n",
    "                                 }) \\\n",
    "  .whenMatchedUpdate(set = {'email' : col('raw.email') \\\n",
    "                                 ,'first_name' : col('raw.first_name') \\\n",
    "                                 ,'last_name' : col('raw.last_name') \\\n",
    "                                 ,'gender' : col('raw.gender') \\\n",
    "                                 ,'username' : col('raw.username') \\\n",
    "                                 }) \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556d58c-2eb3-468e-8778-22ca78e1060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.toDF().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e9198-46a6-482a-a0fe-e7cd8fa9b261",
   "metadata": {},
   "source": [
    "## SPARK SUBMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d91aff8-05fe-431b-b31e-a6a43c8ac5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "spark = SparkSession.builder.appName(\"Aula\").enableHiveSupport().getOrCreate()\n",
    "def loadData (qtde):\n",
    "    list = []\n",
    "    for x in range (qtde):\n",
    "        print(x)\n",
    "        r = requests.get('https://random-data-api.com/api/v2/users')\n",
    "        list.append(r.json())\n",
    "        req = spark.read.json(spark.sparkContext.parallelize(list))\n",
    "        req = req.select( \\\n",
    "         'email' \\\n",
    "        ,'first_name' \\\n",
    "        ,'last_name' \\\n",
    "        ,'gender' \\\n",
    "        ,'id' \\\n",
    "        ,'username' \\\n",
    "                 )\n",
    "    return req\n",
    "\n",
    "df = loadData(10)\n",
    "df.repartition(1).write.parquet('/datalake/raw/api',mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b255cb-5f1e-4a26-a815-9064c11858ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark-submit --master spark://spark-master:7077 --supervise --executor-memory 1G --total-executor-cores 1 programa.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1832aa-22dc-4625-998c-65e15d621ab0",
   "metadata": {},
   "source": [
    "## STREAMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03842df-78f8-4d8a-b939-bcccce752c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import  StructType, StructField, StringType, LongType, DoubleType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import expr, from_json, col, concat\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "spark = SparkSession.builder.appName(\"Aula\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf276c5-611d-406c-9936-6a5a7c44d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#API\n",
    "#https://www.boredapi.com/api/activity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7481f-2c69-4e82-b036-06e761085268",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"activity\", StringType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"participants\", IntegerType()),\n",
    "    StructField(\"price\",DoubleType()),\n",
    "    StructField(\"link\", StringType()),\n",
    "    StructField(\"key\", StringType()),\n",
    "    StructField(\"accessibility\",DoubleType())])\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911fd46-9ba1-4071-b9c1-9d76c7231347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stream dataframe setting kafka server, topic and offset option\n",
    "df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-broker:9092\") # kafka server\n",
    "  .option(\"subscribe\", \"atividade\") # topic\n",
    "  .option(\"startingOffsets\", \"earliest\") # start from beginning \n",
    "  #.option(\"checkpoint\",\"s3a://tmp/checkpoint\")    \n",
    "  .option(\"kafka.group.id\", \"spark3\")\n",
    "  .load() \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0c7b2-6d61-4c7f-912f-b741776fccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bfaf96-b668-447d-9b1c-f60bda3ab89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to Output DataFrame\n",
    "value_df = df.select(from_json(col(\"value\").cast(\"string\"),schema).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc43b66d-17d3-4f16-9883-aca4d73bacde",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df = value_df.selectExpr('value.activity', 'value.type', 'value.participants', 'value.price',\n",
    "                                      'value.price','value.link', 'value.key', 'value.accessibility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64c9a5-3250-4192-af2f-512a4bb9cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd0095e-5d93-4725-b06c-590d6e2e23cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESCREVER EM MEM√ìRIA\n",
    "rawQuery = exploded_df \\\n",
    "        .writeStream \\\n",
    "        .queryName(\"qraw\")\\\n",
    "        .format(\"memory\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f2866-2725-41c7-8139-6a502ba2f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca17af5-7be5-48b3-8686-f3548b236239",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec2931f-4caa-4785-9158-97817d04e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = spark.sql(\"select * from qraw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a18af4a-e2fb-4b65-962a-7d4d007bbabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b876fb-1c4c-41c0-80d8-b91741e2fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bfdf1f-4417-47dd-ad52-7b46cff69ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93cb27e-2539-4942-b7eb-683fe889a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESCREVER EM DISCO\n",
    "output_query = exploded_df.writeStream\\\n",
    "        .format(\"json\")\\\n",
    "        .option(\"path\",\"/datalake/raw/streaming\")\\\n",
    "        .option(\"checkpointLocation\", \"chck-pnt-dir-kh\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .queryName(\"SS Writter\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a114515-289b-496a-b3f7-acd89f7e9e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b0c1b-6ff7-4124-b19b-ee1391a237e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
